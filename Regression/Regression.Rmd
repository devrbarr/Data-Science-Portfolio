---
title: "An Introduction to Simple Regression"
author: "Robert Barr"
date: "2022-11-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, include = TRUE)
```

## Predicting an outcome from a model

Almost if not all data can be predicted using this general equation:

 - outcome~i~ = (model) + error~i~

## Regression Concepts

- What is regression?
  - The statistical analysis studying the relationship between a set of independent variables (explanatory variables) and dependent variables (response variables)
  - Regression Analysis: Fit a model to your data and use it to predict values of the dependent variable (DV) from one or more independent variables (IVs)
    - Simple Regression: predicting an outcome variable from one predictor variable
    - Multiple Regression: Predicting an outcome variable from several predictor variables.
  - Regression versus Correlation
    - Regression analysis develops a statistical model used to **predict** the values of a dependent variable based on the value(s) of independent predictor variable(s).
    - Correlation measures the **intensity** of the relationship between variables
    <center>![Regression vs Correlation](Regression versus Correlation.png "Regression vs Correlation")</center>
  - Types of Regression
    - Linear
    - Nonlinear
  - Using the general equation:
    - outcome~i~ = (linear model) + error~i~
    - linear model:
      - The values defining our 'fit' line
        - slope (gradient) denoted by *b~1~* 
        - The point where the line crosses the vertical axis - knows as the *intercept* *b~0~*
    - The general equation becomes a regression model:
      - Y~i~ = (*b~0~* + *b~1~*X~i~) + Ɛ~i~ (Y~i~ = (β~0~ + β~1~ X~i~) + ε~i~)
        - Y~i~ is what you want to predict
        - X~i~ is the predictor variable
        - *b~i~* is the line slope/gradient
          - Tells us if there is a positive or negative relationship. 
        - *b~0~* is the intercept
        -  Ɛ~i~ represents the difference (residual) between whats predicted versus what was actual
        
        > (**NOTE:** *b~0~* and *b~1~* are known as the **regression coefficients**)
    
## Is my regression good?

- How can you know how well your line will accurately predict data?
  - The method of least squares: The 'line of best fit' is that line of all lines with the least amount of difference between observed and prediction line.
  
  <center>![Line of Best Fit](Line of Best Fit.png "Line of Best Fit")</center>
    
  - Summing the positive and negative differences (residuals) tend to cancel each other out. To prevent this effect, square the differences and then add them up.
    - If the squared residuals is large the line does not represent the data. If the squared differences is small: the line is representative.
    
    - **Method of Least Squares**: Determines the line (of all possible) with the lowest SS.
    
## Determinging Goodness of Fit
After determining the "line of best fit" compare that line using the actual data. 

- Key Terms & Concepts:
  - SS~T~: Total Sum of Squares. How good the mean is as a model of the observed/actual data
  - SS~R~: Residual Sum of Squares. The degree of inaccuracy when the the best model if used to fit the data. 
  - SS~M~: Model Sum of Squares: SS~T~ - SS~R~. 
    - If SS~M~ is large use the mean
    - If SS~M~ is small use the regression model
  - R<sup>2</sup>: R<sup>2</sup> = SS~M~ / SS~R~. The amount of variance in the outcome explained by the model (SS~M~) relative to how much variation
SM there was to explain in the first place (SS~T~).

## Are my predictors any good?

Recall the regression model: Y~i~ = (*b~0~* + *b~1~*X~i~) + Ɛ~i~
 - b~1~ is the predictor variables coefficient. It is the change in the outcome resulting from a unit change in the predictor. A bad model will have the value of zero for it's predictor variable.  - the t-statistic test if the coefficient is 0 ... aka the null hypothesis.
 
## Example: Album Sales
# The lm() function
  - Specify the lm() formula symbolically
    - formula = response **~** terms
      - **response** - the numeric response vector
      - **terms** - a series of terms specifying a line predictor for response
        - A + B: All terms in A plus all terms in B **without duplicates**
        - A : B: The interaction of **all** terms of A with **all* the terms of B
        - A * B: The cross of A and B


```{r }

df_album_sales.1 <- read.delim("Album Sales 1.dat", header = TRUE)
df_album_sales.2 <- read.delim("Album Sales 2.dat", header = TRUE)

str(df_album_sales.1)
summary(df_album_sales.1)

plot(x = df_album_sales.1$adverts, y = df_album_sales.1$sales)
abline(
  lm(formula = df_album_sales.1$sales ~ df_album_sales.1$adverts, na.action =  na.fail), 
  col = 'blue', 
  lwd = 5
)

plot(x = df_album_sales.1$adverts, y = df_album_sales.1$sales)
abline(
  lm(formula = sales ~ adverts, data = df_album_sales.1, na.action =  na.fail), 
  col = 'red', 
  lwd = 5
)


reg_album_sales.1 <- lm(formula = sales ~ adverts, data = df_album_sales.1, na.action = na.fail)
reg_album_sales.1

## The Regression Model
#sales = adverts*b + intercept 
#sales = adverts * 0.09612 + 134.13994 

summary(reg_album_sales.1)

#Overall Fit
# R^2 0.3346
r <- sqrt(0.3346)

```
## Example: Selling Printers
```{r}

df_printer_sales <- read.csv("SellingPrinters.csv", header = TRUE, sep = ";")
str(df_printer_sales)
head(df_printer_sales)
plot(x = df_printer_sales$Sold.Items, y = df_printer_sales$Price, cex = .75, lwd = 5, main = "Scatter Plot: Price as a function of units sold", xlab = "Number Sold", ylab = "Price")

# Covariance
cov(df_printer_sales$Sold.Items, df_printer_sales$Price) # -107.8105

# Standard Deviation
sd(df_printer_sales$Sold.Items)
sd(df_printer_sales$Price)

# Correlation Coefficient
cor(df_printer_sales$Sold.Items, df_printer_sales$Price) # -0.9625725

# Simple Linear Regression
reg_printer_sales <- lm(formula =df_printer_sales$Price ~ df_printer_sales$Sold.Items, na.action = na.fail)
reg_printer_sales

summary(reg_printer_sales)

# Printer a scatter plot and add a best of fit line
plot(x = df_printer_sales$Sold.Items, y = df_printer_sales$Price, cex = .75, lwd = 5, main = "Scatter Plot: Price as a function of units sold", xlab = "Number Sold", ylab = "Price")
abline(reg_printer_sales)


```
## Example: Licensed Drivers
```{r}
# install.packages("readxl")
library(readxl)

dat_LicDrivers <- read_excel("LicensedDrivers.xlsx")
str(dat_LicDrivers)

# Fit a regression model to predict number of licensed drivers in the US in millions by using Year
reg_LicDrivers <- lm(dat_LicDrivers$LicensedDrivers ~ dat_LicDrivers$Year, na.action = na.fail)
reg_LicDrivers

# Number of Licensed Drivers = -4844.307 + 2.517 * Year

# Plot
plot(x = dat_LicDrivers$Year, y = dat_LicDrivers$LicensedDrivers, cex = .25, lwd = 5)
abline(reg_LicDrivers, col = 'red')

summary(reg_LicDrivers)


```


## Resources
  - [Discovering statistics using R, Sage Publishing](https://studysites.sagepub.com/dsur/main.htm "Link to Book")